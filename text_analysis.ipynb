{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sergio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16528/3865123116.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wordnet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m## Other\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "## General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "## Text Analysis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in data\n",
    "with open('data/movies_text.pkl', 'rb') as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Action', 'Sci-Fi']\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning data for text preprocessing\n",
    "\n",
    "### Cleaning helper function\n",
    "\n",
    "\n",
    "temp = data.actors[0:10].copy()\n",
    "\n",
    "actors = list(data.actors)\n",
    "\n",
    "for elem in temp:\n",
    "    actors[actors.index(elem)] = re.sub(\"\\[|\\]|\\'| \", '', elem).split(',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_data = data.copy().drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "new_data.genres = new_data.genres.fillna('None')\n",
    "\n",
    "def cleaner(x):\n",
    "    to_replace = \"\\[|\\]|\\'|:|directors|\\\"| \"\n",
    "\n",
    "    x.actors = set(re.sub(to_replace, '', x.actors).lower().split(','))\n",
    "    x.genres = set(re.sub(to_replace, '', x.genres).lower().split(','))\n",
    "    \n",
    "    ## Text processing\n",
    "\n",
    "    ### Tokenizing and removing stopwords in description\n",
    "    processed_desc = [word for word in word_tokenize(x.description) if word not in stopwords.words('english')]\n",
    "    \n",
    "    ### Lemmatizing words in processed description\n",
    "    lem = WordNetLemmatizer()\n",
    "    processed_desc = [lem.lemmatize(word) for word in processed_desc]\n",
    "\n",
    "    ### removing punctuation and converting back into a string\n",
    "    processed_desc = ' '.join([word for word in processed_desc if word.isalnum()])\n",
    "\n",
    "    ## Adding processed descriptions to row\n",
    "    processed_desc_s = pd.Series([processed_desc], index = ['processed_desc'])\n",
    "    x = x.append(processed_desc_s)\n",
    "    \n",
    "    return x\n",
    "\n",
    "movies = new_data.apply(cleaner, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Using `textblob` for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding sentiment polarity scores as a column to dataset\n",
    "movies['desc_polarity'] = movies.processed_desc.apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Similarity\n",
    "Using `sklearn` and `tf-idf` to measure similarity between descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tf_idf_processor(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    documents = [text1, text2]\n",
    "\n",
    "    results = vectorizer.fit_transform(documents)\n",
    "\n",
    "    similarity = cosine_similarity(results[0:1], results[1:]).flatten()\n",
    "\n",
    "    return similarity[0]\n",
    "\n",
    "\n",
    "def dataframe_processor(df=):\n",
    "    m = df.movie\n",
    "\n",
    "    cos_sim_df = pd.DataFrame()\n",
    "\n",
    "    for val in df.values:\n",
    "        similarities = [tf_idf_processor(val[9], x[9]) for x in df.values]\n",
    "        series = pd.Series(similarities)\n",
    "        cos_sim_df = cos_sim_df.append(series, ignore_index = True)\n",
    "    \n",
    "    cos_sim_df.columns = m\n",
    "    cos_sim_df.index = m\n",
    "    cos_sim_df = cos_sim_df.reset_index(drop = False).rename_axis(None)\n",
    "    \n",
    "    return cos_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046043606474267834"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_processor(movies.iloc[0]['processed_desc'], movies.iloc[1]['processed_desc'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3d1d18059ea17a6584414eb81c3831d8a89c5b0bdf508eff04ef4bcce843af3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
